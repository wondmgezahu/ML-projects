{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Question 1\n",
    "import numpy as np\n",
    "from scipy . stats import multivariate_normal as mvn\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "import math\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV,KFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def data(n):\n",
    "  label = np.zeros((1,n))\n",
    "  for i in range(n):\n",
    "    label[0,i]= np.random.choice([0,1],p=prior) # equal prior\n",
    "    \n",
    "  x = np.zeros ((features , n))\n",
    "\n",
    "  for index in range (n) :\n",
    "    theta=np.random.uniform(theta_margin[0],theta_margin[1])\n",
    "    temp=np.array([np.cos(theta),np.sin(theta)])\n",
    "\n",
    "    if label [0,index] == 0:#sample from class -1\n",
    "      x [:,index] = 2*(temp)+mvn(m1_n,c1_n).rvs(1)\n",
    "    else:# from class +1\n",
    "      x [:,index] = 4*(temp)+mvn(m2_n,c2_n).rvs(1)\n",
    "\n",
    "  return x,label \n",
    "  \n",
    "  \n",
    "def split_data(data, folds):\n",
    "  data_split = []\n",
    "  random.seed(1)\n",
    "  data_temp = list(data)\n",
    "  fold_size = int(len(data) / folds)\n",
    "  for i in range(folds):\n",
    "    fold = list()\n",
    "    while len(fold) < fold_size:\n",
    "      rand_index = random.randrange(len(data_temp))\n",
    "      fold.append(data_temp.pop(rand_index))\n",
    "    data_split.append(fold)\n",
    "  return data_split\n",
    "\n",
    "def test_train_split(folds,i):\n",
    "  test=np.array(folds.pop(i)) \n",
    "  train=np.array(folds) \n",
    "\n",
    "  xtest=test[:,0:2]\n",
    "  ytest=test[:,2]\n",
    "  xtrain=train[:,:,0:2].reshape(-1,2)\n",
    "  ytrain=train[:,:,2].reshape(-1)\n",
    "\n",
    "  return xtrain,ytrain,xtest,ytest  \n",
    "\n",
    "def MLP(sample_type):\n",
    "\n",
    "  error_mat=np.zeros((len(percept_list),len(activations)))\n",
    "  accuracy_mat=np.zeros((len(percept_list),len(activations)))\n",
    "\n",
    "  data_concat=np.hstack((X_train,Y_train))\n",
    "\n",
    "  for activ in range(len(activations)):\n",
    "\n",
    "    for idx,percpt in enumerate(percept_list):\n",
    "      err_fold=[]\n",
    "      acc_fold=[]\n",
    "      \n",
    "      for i in range(num_folds):\n",
    "        X_train_,Y_train_,X_test_,Y_test_=test_train_split(split_data(data_concat,num_folds),i)\n",
    "        input_shape = (features,)    \n",
    "        Y_train_encod=to_categorical(Y_train_,num_class)  \n",
    "        model = tf.keras.models.Sequential()\n",
    "\n",
    "        model.add(Dense(percpt,kernel_initializer='random_uniform', input_shape=input_shape, activation=activations[activ]))\n",
    "        model.add(Dense(num_class, kernel_initializer='random_uniform', activation='softmax'))\n",
    "\n",
    "        model.compile( optimizer='Adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(X_train_,Y_train_encod, epochs=100, batch_size=32, verbose=0) \n",
    "        temp_y=model.predict(X_test_) \n",
    "        y_pred_=np.argmax(temp_y, axis=1)\n",
    "        tmt=to_categorical(Y_test_,num_class)\n",
    "        y_test=np.argmax(tmt, axis=1)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred_,labels=[0.0,1.0])\n",
    "        temp=(cm[0,0]+cm[1,1])/cm.sum()\n",
    "        err_fold.append(1-temp)\n",
    "        acc_fold.append(temp)\n",
    "\n",
    "      error_mat[idx][activ]=(np.mean(err_fold)) # choose the perceptron based on the min of error\n",
    "      accuracy_mat[idx][activ]=(np.mean(acc_fold))\n",
    "\n",
    "   # find the perceptron with minimum error or loss\n",
    "  opt_percerptron=np.where(error_mat==error_mat.flat[np.argmin(error_mat)])\n",
    "  optimal_perceptron=percept_list[int(opt_percerptron[0][0])]\n",
    " \n",
    "  print(f'optimal number of neurons:{optimal_perceptron}')\n",
    "\n",
    "  input_shape = (features,)\n",
    "  print(f'Feature shape: {input_shape}')\n",
    "\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(Dense(optimal_perceptron,kernel_initializer='random_uniform', input_shape=input_shape, activation='relu'))\n",
    "  model.add(Dense(1, kernel_initializer='random_uniform',activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "  model.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=1) \n",
    "\n",
    "  # # Test the model after training\n",
    "  test_results = model.evaluate(X_test, Y_test, verbose=1)\n",
    "  print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}')\n",
    "  p_pred = model.predict(X_test)\n",
    "  p_pred = p_pred.flatten()\n",
    "  # extract the predicted class labels\n",
    "  y_pred = np.where(p_pred > 0.5, 1, 0)\n",
    "  cm=confusion_matrix(Y_test, y_pred)\n",
    "  normalized_cm=cm / cm.astype(float).sum(axis=1)\n",
    "  error = (1- (cm[0,0]+cm[1,1])/cm.sum())\n",
    "\n",
    "  print(f'minimum probability of error:{error}')\n",
    "  print(f'confusion matrix:{cm}')\n",
    "  print(f'Normalized confusion matrix:{normalized_cm}')\n",
    "\n",
    "\n",
    "  min1, max1 = X_test[:, 0].min()-1, X_test[:, 0].max()+1\n",
    "  min2, max2 = X_test[:, 1].min()-1, X_test[:, 1].max()+1\n",
    "  # define the x and y scale\n",
    "  x1grid = np.arange(min1, max1, 0.1)\n",
    "  x2grid = np.arange(min2, max2, 0.1)\n",
    " \n",
    "  xx, yy = np.meshgrid(x1grid, x2grid)\n",
    "\n",
    "  r1, r2 = xx.flatten(), yy.flatten()\n",
    "  r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n",
    "  # horizontal stack vectors to create x1,x2 input for the model\n",
    "  grid = np.hstack((r1,r2))\n",
    "  yhat = model.predict(grid)\n",
    "  contour_val=[xx,yy,yhat]  \n",
    "  return error_mat,contour_val,y_pred\n",
    "\n",
    "def SVM(sample_type):\n",
    "\n",
    "  grid = dict(kernel=kernel,C=C_list,gamma= gamma_list )\n",
    "  cross_val = KFold(n_splits=10)\n",
    "  grid_object = GridSearchCV(estimator=SVC(), param_grid=grid, cv=cross_val, scoring='accuracy')\n",
    "  grid_result = grid_object.fit(X_train, Y_train.reshape(-1))\n",
    "  print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "  svc = SVC(kernel = 'rbf', C = 100, gamma=0.01 )\n",
    "\n",
    "  svc.fit(X_train,Y_train)\n",
    "  y_pred=svc.predict(X_test)\n",
    "\n",
    "  cm=confusion_matrix(Y_test, y_pred)\n",
    "  normalized_cm=cm / cm.astype(float).sum(axis=1)\n",
    "  error = (1- (cm[0,0]+cm[1,1])/cm.sum())\n",
    "\n",
    "  print(classification_report(Y_test, y_pred)) \n",
    "  print('Model accuracy score : {0:0.4f}'. format(accuracy_score(Y_test, y_pred)))\n",
    "\n",
    "  grid_temp=list(grid_result.cv_results_['mean_test_score']*100)\n",
    "\n",
    "  min1, max1 = X_test[:, 0].min()-1, X_test[:, 0].max()+1\n",
    "  min2, max2 = X_test[:, 1].min()-1, X_test[:, 1].max()+1\n",
    "\n",
    "  x1grid = np.arange(min1, max1, 0.1)\n",
    "  x2grid = np.arange(min2, max2, 0.1)\n",
    "\n",
    "  xx, yy = np.meshgrid(x1grid, x2grid)\n",
    "\n",
    "  r1, r2 = xx.flatten(), yy.flatten()\n",
    "  r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n",
    " \n",
    "  grid = np.hstack((r1,r2))\n",
    "  yhat = svc.predict(grid)\n",
    "  contour_val=[xx,yy,yhat] \n",
    "\n",
    "  return contour_val,y_pred,grid_temp\n",
    "\n",
    "\n",
    "def plot_results(sample_type,classifier_type):\n",
    "\n",
    "\n",
    "  if classifier_type=='MLP':\n",
    "    error_matt,contour_val,y_pred=MLP(sample_type)\n",
    "      \n",
    "      #plot min error vs number of perceptrons\n",
    "  \n",
    "    tt=np.array(percept_list)\n",
    "    bar1=error_matt[:,0]\n",
    "   \n",
    "    fig = plt.figure(figsize = (10, 7))\n",
    "    ax2 = plt.axes()\n",
    "    ax2.scatter(tt,bar1)\n",
    "    ax2.plot(tt,bar1)\n",
    "\n",
    "    plt.legend(activations)\n",
    "    plt.xlabel('Number of epochs') \n",
    "    plt.ylabel('Average minimum Probability of Error') \n",
    "    plt.title('Minimum Error for different number of epochs')\n",
    "    plt.show()\n",
    "\n",
    "  elif classifier_type=='SVM':\n",
    "    contour_val,y_pred,grid_temp=SVM(sample_type)\n",
    "\n",
    "\n",
    "    # accuracy percentage SVM\n",
    "    \n",
    "\n",
    "    plt.plot(gamma_list,grid_temp[0:6])\n",
    "    plt.plot(gamma_list,grid_temp[6:12])\n",
    "    plt.plot(gamma_list,grid_temp[12:18])\n",
    "    plt.plot(gamma_list,grid_temp[18:24])\n",
    "    plt.plot(gamma_list,grid_temp[24:30])\n",
    "    plt.plot(gamma_list,grid_temp[30:36])\n",
    "    plt.xscale('log')\n",
    "\n",
    "    plt.xlabel(\"Values of Gamma\")\n",
    "    plt.ylabel(\"Accuracy  in %\")\n",
    "    plt.legend(['C= 1000','C = 100','C = 10','C = 1.0','C = 0.1','C = 0.01'])\n",
    "    plt.title(\"Accuracy for all combinations of C and gamma\")\n",
    "    plt.show()\n",
    "# actual data distribution for test data\n",
    "  x0=[X_test[i] for i in range(samples[1]) if Y_test[i]==0]\n",
    "  x1=[X_test[i] for i in range(samples[1]) if Y_test[i]==1]\n",
    "\n",
    "  s1=[2 for i in range(len(x0))]\n",
    "\n",
    "  s2=[2 for i in range(len(x1))]\n",
    "\n",
    "\n",
    "  plt.scatter((np.array(x0))[:,0],(np.array(x0))[:,1],s1)\n",
    "  plt.scatter((np.array(x1))[:,0],(np.array(x1))[:,1],s2)\n",
    "\n",
    "  plt.legend(['class -1','class +1'])\n",
    "  plt.title('Actual Data Distribution')\n",
    "  plt.show\n",
    "  \n",
    "  # contour plot of decision boundary and the classified data \n",
    "  x_temp=contour_val[0]\n",
    "  y_temp=contour_val[1]\n",
    "  \n",
    "\n",
    "  x00t = [i for i in range(10000) if (Y_test[i] == 0 and y_pred[i] == 0)]\n",
    "  x01t = [i for i in range(10000) if (Y_test[i] == 0 and y_pred[i] == 1)]#fp\n",
    "  x10t = [i for i in range(10000) if (Y_test[i] == 1 and y_pred[i] == 0)]#fN\n",
    "  x11t = [i for i in range(10000) if (Y_test[i] == 1 and y_pred[i] == 1)]\n",
    "\n",
    "  plt.contourf(x_temp, y_temp, contour_val[2].reshape(x_temp.shape), cmap='gist_heat')\n",
    "  plt.plot(X_test[x00t,0],X_test[x00t,1],'_',color ='g', markersize = 1.95)#g\n",
    "  plt.plot(X_test[x01t,0],X_test[x01t,1],'_',color = 'r', markersize = 1.95)#r\n",
    "  plt.plot(X_test[x11t,0],X_test[x11t,1],'x',color ='g', markersize = 1.85)#g\n",
    "  plt.plot(X_test[x10t,0],X_test[x10t,1],'x',color = 'r', markersize = 1.85)#r\n",
    "\n",
    "\n",
    "  plt.legend(['class 0 correctly labeled','class 0 wrongly labeled','class 1 correctly labeled','class 1 wrongly labeled'])\n",
    "  plt.xlabel(\"x1\")\n",
    "  plt.ylabel(\"x2\")\n",
    "  plt.show()\n",
    "\n",
    "def main():\n",
    "  plot_results(samples[0],'MLP')\n",
    "  plot_results(samples[0],'SVM')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  activations=['relu']\n",
    "  num_folds=10\n",
    "  num_class=2\n",
    "  features=2\n",
    "  samples=[1000,10000]\n",
    "  m1_n=np.zeros(2)\n",
    "  m2_n=np.zeros(2)\n",
    "\n",
    "  c1_n=np.eye(2)\n",
    "  c2_n=np.eye(2)\n",
    "\n",
    "  theta_margin=[-(np.pi),(np.pi)]\n",
    "  mu_vector=[m1_n,m2_n]\n",
    "  sigma_vector=[c1_n,c2_n]\n",
    "  prior=np.array([0.5,0.5])\n",
    "  percept_list=np.logspace(1,3,num = 15,endpoint = True,base = 5,dtype = int)\n",
    "  percept_list=percept_list.tolist()\n",
    "\n",
    "  kernel = ['rbf'] # can addd multiple kernels \n",
    "  C_list = [1000, 100, 10, 1.0, 0.1, 0.01]\n",
    "  gamma_list = [ 0.01,0.1, 1,10,100,1000]\n",
    "  data_train,label_train=data(samples[0])\n",
    "  data_test,label_test=data(samples[1])\n",
    "\n",
    "  X_train=data_train.T\n",
    "  X_test=data_test.T\n",
    "\n",
    "  Y_train =label_train.T\n",
    "  Y_test = label_test.T\n",
    "\n",
    "  X_train = X_train.reshape(X_train.shape[0], features)\n",
    "  X_test = X_test.reshape(X_test.shape[0], features)\n",
    "\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy . stats import multivariate_normal as mvn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def GMM():\n",
    "\n",
    "  num_fold=10\n",
    "  gmm_list=[1,2,3,4,5,6,8,10]\n",
    "  data_image = cv2.imread('/content/drive/MyDrive/119082.jpg')\n",
    "  feature_vector=np.zeros((data_image.shape[0] * data_image.shape[1],5))\n",
    "  for i in range(data_image.shape[0]):\n",
    "    for j in range(data_image.shape[1]):\n",
    "      feature_vector[i*data_image.shape[1] + j,0]=i\n",
    "      feature_vector[i*data_image.shape[1] + j,1]=j\n",
    "      feature_vector[i*data_image.shape[1] + j,2]=data_image[i,j,2]\n",
    "      feature_vector[i*data_image.shape[1] + j,3]=data_image[i,j,1]\n",
    "      feature_vector[i*data_image.shape[1] + j,4]=data_image[i,j,0]\n",
    "      \n",
    "  min_max=minmax_scale(feature_vector)\n",
    "  gmm_avg=np.zeros((len(gmm_list)))\n",
    "  gmm_var=np.zeros((len(gmm_list)))\n",
    "  temp=[]\n",
    "  for idx,i in enumerate(gmm_list):\n",
    "    gmm=GaussianMixture(i,covariance_type='full',random_state=None)\n",
    "    scor=cross_val_score(gmm,min_max,cv=num_fold) # this is the log likelhood score\n",
    "    avg_scor=np.mean(scor)\n",
    "    var_scor=np.std(scor)\n",
    "\n",
    "    gmm_avg[idx]=avg_scor\n",
    "    gmm_var[idx]=var_scor\n",
    "  \n",
    "    temp.append(avg_scor)\n",
    " \n",
    "  model_sel=np.argmax(temp)\n",
    "  model_sel=gmm_list[model_sel]\n",
    "  vv=np.array(temp)\n",
    "\n",
    "  \n",
    "  plt.plot(gmm_list,vv,c='b', mfc='red',marker='o')\n",
    "  plt.xlabel('model order')\n",
    "  plt.ylabel(f'log likelihood')\n",
    "  plt.title(f'log likelihood under different GMM model orders',fontsize=12.)\n",
    "  plt.show()\n",
    "\n",
    "  gmm_model=GaussianMixture(model_sel,covariance_type='full').fit(data_image.reshape((-1,3)))\n",
    "  gmm_labels=gmm_model.predict(data_image.reshape((-1,3)))\n",
    "  \n",
    "  segmented=gmm_labels.reshape((data_image.shape)[0],(data_image.shape)[1])\n",
    "  cv2.imwrite('segment.jpg',segmented)\n",
    "  plt.imshow(np.array(segmented))\n",
    "\n",
    "GMM()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
