{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Question 1\n",
    "import matplotlib . pyplot as plt\n",
    "import numpy as np\n",
    "from scipy . stats import multivariate_normal as mvn\n",
    "from collections import Iterable\n",
    "\n",
    "\n",
    "features = 2\n",
    "samples = [20,200,2000,10000]\n",
    "mean_01=np.array([3,0])\n",
    "mean_02=np.array([0,3])\n",
    "mean_1=np.array([2,2])\n",
    "\n",
    "cov_01=np.array([[2,0],[0,1]])\n",
    "cov_02=np.array([[1,0],[0,2]])\n",
    "cov_1=np.array([[1,0],[0,1]])\n",
    "\n",
    "prior = [0.65, 0.35]\n",
    "fpr = [] \n",
    "tpr = []\n",
    "\n",
    "P_error = [] \n",
    "gamma_list = []\n",
    "\n",
    "#part one \n",
    "\n",
    "def data(n):\n",
    "  label = np.zeros ((3, n ))\n",
    "  label [0 , :] = (np.random.uniform (0,1,n)>= prior[0]).astype (int)\n",
    "  for i in range(n):\n",
    "     p=np.random.uniform(0,1)\n",
    "     if p>=prior[0]:\n",
    "       label[0,i]=1\n",
    "     else:\n",
    "       label[0,i]=0    \n",
    "  x = np.zeros ((features , n))\n",
    "  for index in range (n) :\n",
    "    if label [0,index] == 0:\n",
    "      w=np.random.uniform (0,1)\n",
    "      if w>=0.5:\n",
    "        x [:,index] = mvn(mean_01.reshape(2,),cov_01).rvs(1)\n",
    "      else:\n",
    "        x [:,index] = mvn(mean_02.reshape(2,),cov_02).rvs(1) \n",
    "    else:\n",
    "      x [:,index] = mvn(mean_1.reshape(2,),cov_1).rvs(1) \n",
    "  return x,label\n",
    "\n",
    "Data_train_20,label_20=data(samples[0])\n",
    "Data_train_200,label_200=data(samples[1]) \n",
    "Data_train_2000,label_2000=data(samples[2])\n",
    "Data_validate_10000,label_10000=data(samples[3]) \n",
    "\n",
    "logValpdf0=np.zeros(samples[3])\n",
    "\n",
    "for i in range(samples[3]):\n",
    "  pdf01= np.log(mvn.pdf(Data_validate_10000[:,i].T,mean = mean_01,cov = cov_01))\n",
    "  pdf02= np.log(mvn.pdf(Data_validate_10000[:,i].T,mean = mean_02,cov = cov_02))\n",
    "  logValpdf0[i] = 0.5*pdf01+0.5*pdf02\n",
    "\n",
    "logValpdf1= np.log(mvn.pdf(Data_validate_10000.T,mean=mean_1, cov = cov_1))    \n",
    "discriminant_score = logValpdf1 - logValpdf0\n",
    "class0_count = float(list(label_10000[0,:]).count(0)) # number of samples for class 0\n",
    "class1_count = float(list(label_10000[0,:]).count(1)) # number of samples for class 1\n",
    "\n",
    "tau=(sorted(discriminant_score[np.array(discriminant_score[:].astype(float) >=0)]))\n",
    "mid_tau=np.array([tau[0]-100,(tau[0:(len(tau)-1)]+(np.diff(tau))/2).tolist(),tau[len(tau)-1]+100])\n",
    "\n",
    "def flatten(lis):\n",
    "     for item in lis:\n",
    "         if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "             for x in flatten(item):\n",
    "                 yield x\n",
    "         else:        \n",
    "             yield item\n",
    "mid_tau=list(flatten(mid_tau.tolist()))\n",
    "\n",
    "\n",
    "for gamma in mid_tau:#gamma_list\n",
    " label_10000[1,:] = (discriminant_score>=gamma)#.astype(int)#np.log(gamma)\n",
    "\n",
    " x01 = [i for i in range(label_10000.shape[1]) if (label_10000[1,i] == 1 and label_10000[0,i] == 0)]\n",
    " x11 = [i for i in range(label_10000.shape[1]) if (label_10000[1,i] == 1 and label_10000[0,i] == 1)]\n",
    " tpr.append(len(x11)/class1_count)\n",
    " fpr.append(len(x01)/class0_count)\n",
    " P_error.append(  (len(x01)/class0_count)*prior[0] + (1-len(x11)/class1_count)*prior[1])\n",
    "\n",
    "\n",
    "minimum_Perror=min(P_error)\n",
    "min_idx=np.argmin(P_error)\n",
    "fpr_theory=[]\n",
    "tpr_theory=[]\n",
    "p_thry=[]\n",
    "\n",
    "label_10000[2,:] = (discriminant_score>= np.log(prior[0]/prior[1])).astype(int)\n",
    "x00t = [i for i in range(label_10000.shape[1]) if (label_10000[0,i] == 0 and label_10000[2,i] == 0)]\n",
    "x01t = [i for i in range(label_10000.shape[1]) if (label_10000[0,i] == 0 and label_10000[2,i] == 1)]#fp\n",
    "x10t = [i for i in range(label_10000.shape[1]) if (label_10000[0,i] == 1 and label_10000[2,i] == 0)]#fN\n",
    "x11t = [i for i in range(label_10000.shape[1]) if (label_10000[0,i] == 1 and label_10000[2,i] == 1)]\n",
    "fpr_theory . append (len( x01t ) / class0_count )\n",
    "tpr_theory . append (len( x11t ) / class1_count )\n",
    "\n",
    "p_thry.append(  (len(x01t)/class0_count)*prior[0] + (1-len(x11t)/class1_count)*prior[1])\n",
    "\n",
    "print('Optimal threshold {}'.format(mid_tau[min_idx]))#change here\n",
    "print('TPR at minimum probability :{}'.format ( tpr [ min_idx ]))\n",
    "print('FPR at minimum probability :{}'.format ( fpr [ min_idx ]))\n",
    "print('Minimum Probability Error:{}'.format(minimum_Perror))\n",
    "print('Minimum Theoretical Probability Error:{}'.format(min(p_thry)))\n",
    "\n",
    "#plot ROC\n",
    "\n",
    "plt.plot(fpr,tpr,color = 'red' )\n",
    "plt.plot(fpr[min_idx],tpr[min_idx],'o',color='k')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(['ROC','Experimental min Error'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#plot the predicted labels\n",
    "plt.plot(Data_validate_10000[0,x00t],Data_validate_10000[1,x00t],'.',color ='g', markersize = 6)\n",
    "plt.plot(Data_validate_10000[0,x01t],Data_validate_10000[1,x01t],'.',color = 'r', markersize = 6)\n",
    "plt.plot(Data_validate_10000[0,x11t],Data_validate_10000[1,x11t],'+',color ='g', markersize = 6)\n",
    "plt.plot(Data_validate_10000[0,x10t],Data_validate_10000[1,x10t],'+',color = 'r', markersize = 6)\n",
    "plt.legend(['class 0 correctly classified','class 0 wrongly classified','class 1 correctly classified','class 1 wrongly classified'])\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "\n",
    "horizontalGrid = np.linspace(np.floor(min(Data_validate_10000[0,:])),np.ceil(max(Data_validate_10000[0,:])),100)\n",
    "verticalGrid = np.linspace(np.floor(min(Data_validate_10000[1,:])),np.ceil(max(Data_validate_10000[1,:])),100)\n",
    "\n",
    "dsg = np.zeros((100,100))\n",
    "a = np.array(np.meshgrid(horizontalGrid,verticalGrid))\n",
    "for i in range(100):\n",
    "  for j in range(100):\n",
    "    ww=np.random.uniform(0,1)\n",
    "    p = mvn.pdf(np.array([a[0][i][j], a[1][i][j]]),mean=mean_1, cov = cov_1)\n",
    "    q1=mvn.pdf(np.array([a[0][i][j], a[1][i][j]]),mean=mean_01, cov = cov_01)\n",
    "    q2=mvn.pdf(np.array([a[0][i][j], a[1][i][j]]),mean=mean_02, cov = cov_02)\n",
    "    q=0.5*q1+0.5*q2\n",
    "    dsg[i][j] = np.log(q) - np.log(p) - np.log(prior[0]/prior[1])\n",
    "plt.contour(a[0],a[1],dsg,levels=[0])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x0 = [i for i in range(label_10000.shape[1]) if (label_10000[0,i] == 0)]\n",
    "x1 = [i for i in range(label_10000.shape[1]) if (label_10000[0,i] == 1 )]\n",
    "plt.plot(Data_validate_10000[0,x0],Data_validate_10000[1,x0],'+')\n",
    "plt.plot(Data_validate_10000[0,x1],Data_validate_10000[1,x1],'.')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title(\"Actual Data distribution\")\n",
    "plt.legend(['Class 0','Class 1'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#part 2\n",
    "def linear_reg(xdata,alpha,max_iteration,ydata,xtest):\n",
    "  z=np.vstack((np.ones(xdata.shape[1]),xdata))\n",
    "  w=np.zeros((3,1))\n",
    "\n",
    "  for i in range(max_iteration):\n",
    "    h=1/(1+np.exp(-(np.dot(w.T,z))))\n",
    "    grad=(1/float(z.shape[1])) * np.dot(z,(h-ydata[0]).T)\n",
    "    w=w-alpha*grad\n",
    "  z=np.vstack((np.ones(xtest.shape[1]),xtest))\n",
    "  decisions=np.zeros((1,xtest.shape[1]))\n",
    "  h=1/(1+np.exp(-(np.dot(w.T,z)))) \n",
    "  decisions[0,:]=(h[0,:]>=0.5).astype(int)\n",
    "  return w,decisions\n",
    "def quadratic_reg(xdata,alpha,max_iteration,ydata,xtest):\n",
    "\n",
    "  z = np.vstack((np.ones(xdata.shape[1]),xdata[0],xdata[1],xdata[0]*xdata[0],xdata[0]*xdata[1], xdata[1]*xdata[1]))\n",
    "  w = np.zeros((6,1))\n",
    "  for i in range(max_iteration):\n",
    "    h=1/(1+np.exp(-(np.dot(w.T,z))))\n",
    "    grad=(1/float(z.shape[1])) * np.dot(z,(h-ydata[0]).T)\n",
    "    w=w-alpha*grad\n",
    "  z = np.vstack((np.ones(xtest.shape[1]),xtest[0],xtest[1],xtest[0]*xtest[0],xtest[0]*xtest[1], xtest[1]*xtest[1]))\n",
    "  decisions = np.zeros((1,xtest.shape[1]))\n",
    "  h=1/(1+np.exp(-(np.dot(w.T,z)))) \n",
    "  decisions[0,:]=(h[0,:]>=0.5).astype(int)\n",
    "  return w,decisions\n",
    "\n",
    "\n",
    "alpha=0.05\n",
    "max_iter=2000\n",
    "\n",
    "w_20,decisions_20=linear_reg(Data_train_20,alpha,max_iter,label_20,Data_validate_10000)\n",
    "w_200,decisions_200=linear_reg(Data_train_200,alpha,max_iter,label_200,Data_validate_10000)\n",
    "w_2000,decisions_2000=linear_reg(Data_train_2000,alpha,max_iter,label_2000,Data_validate_10000)\n",
    "\n",
    "w_20q,decisions_20q=quadratic_reg(Data_train_20,alpha,max_iter,label_20,Data_validate_10000)\n",
    "w_200q,decisions_200q=quadratic_reg(Data_train_200,alpha,max_iter,label_200,Data_validate_10000)\n",
    "w_2000q,decisions_2000q=quadratic_reg(Data_train_2000,alpha,max_iter,label_2000,Data_validate_10000)\n",
    "\n",
    "def contour_plot(typ,w_par):\n",
    "  horizontalGrid = np.linspace(np.floor(min(Data_validate_10000[0,:])),np.ceil(max(Data_validate_10000[0,:])),100)\n",
    "  verticalGrid = np.linspace(np.floor(min(Data_validate_10000[1,:])),np.ceil(max(Data_validate_10000[1,:])),100)\n",
    "  dsg = np.zeros((100,100))\n",
    "  a = np.array(np.meshgrid(horizontalGrid,verticalGrid))\n",
    "  for i in range(100):\n",
    "    for j in range(100):\n",
    "      x1 = a[0][i][j]\n",
    "      x2 = a[1][i][j]\n",
    "      if typ==0:\n",
    "        z = np.c_[1,x1,x2].T\n",
    "      else:\n",
    "        z = np.c_[1,x1,x2,pow(x1,2),x1*x2,pow(x2,2)].T\n",
    "      dsg[i][j] = np.sum(np.dot(w_par.T,z))\n",
    "  plt.contour(a[0],a[1],dsg,levels=[0])\n",
    "  plt.show()\n",
    "\n",
    "def plots(Data_train,label_train,decision,w_val,typ):\n",
    "  P_err=[]\n",
    "\n",
    "  #actual data plots\n",
    "  x0 = [i for i in range(label_train.shape[1]) if (label_train[0,i] == 0)]\n",
    "  x1 = [i for i in range(label_train.shape[1]) if (label_train[0,i] == 1 )]\n",
    "\n",
    "  plt.plot(Data_train[0,x0],Data_train[1,x0],'+')\n",
    "  plt.plot(Data_train[0,x1],Data_train[1,x1],'.')\n",
    "  plt.xlabel('x1')\n",
    "  plt.ylabel('x2')\n",
    "  plt.title(\"Actual Data distribution\")\n",
    "  plt.legend(['Class 0','Class 1'])\n",
    "  if typ==0:#linear\n",
    "    contour_plot(0,w_val)\n",
    "  else:\n",
    "    contour_plot(1,w_val)\n",
    "\n",
    "  #plot the classified data\n",
    "\n",
    "  x00 = [i for i in range(label_10000.shape[1]) if (label_10000[0,i] == 0 and decision[0,i] == 0)]\n",
    "  x01 = [i for i in range(label_10000.shape[1]) if (label_10000[0,i] == 0 and decision[0,i] == 1)]#FP\n",
    "  x10 = [i for i in range(label_10000.shape[1]) if (label_10000[0,i] == 1 and decision[0,i] == 0)]\n",
    "  x11 = [i for i in range(label_10000.shape[1]) if (label_10000[0,i] == 1 and decision[0,i] == 1)]#TP\n",
    "  P_err.append(  (len(x01)/class0_count)*prior[0] + (1-len(x11)/class1_count)*prior[1])\n",
    "  print(min(P_err))\n",
    "\n",
    "  plt.plot(Data_validate_10000[0,x00],Data_validate_10000[1,x00],'.',color ='g', markersize = 6)\n",
    "  plt.plot(Data_validate_10000[0,x01],Data_validate_10000[1,x01],'.',color = 'r', markersize = 6)\n",
    "  plt.plot(Data_validate_10000[0,x11],Data_validate_10000[1,x11],'+',color ='g', markersize = 6)\n",
    "  plt.plot(Data_validate_10000[0,x10],Data_validate_10000[1,x10],'+',color = 'r', markersize = 6)\n",
    "  plt.legend(['class 0 correctly classified','class 0 wrongly classified','class 1 correctly classified','class 1 wrongly classified'])\n",
    "  plt.xlabel(\"x1\")\n",
    "  plt.ylabel(\"x2\")\n",
    " \n",
    "  if typ==0:#linear\n",
    "    contour_plot(0,w_val)\n",
    "  else:\n",
    "    contour_plot(1,w_val)\n",
    "\n",
    "\n",
    "plots(Data_train_20,label_20,decisions_20,w_20,0)\n",
    "plots(Data_train_20,label_20,decisions_20q,w_20q,1)\n",
    "plots(Data_train_200,label_200,decisions_200,w_200,0)\n",
    "plots(Data_train_200,label_200,decisions_200q,w_200q,1)\n",
    "plots(Data_train_2000,label_2000,decisions_2000,w_2000,0)\n",
    "plots(Data_train_2000,label_2000,decisions_2000q,w_2000q,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def generateData(N):\n",
    "    gmmParameters = {}\n",
    "    gmmParameters['priors'] = [.3,.4,.3] # priors should be a row vector\n",
    "    gmmParameters['meanVectors'] = np.array([[-10, 0, 10], [0, 0, 0], [10, 0, -10]])\n",
    "    gmmParameters['covMatrices'] = np.zeros((3, 3, 3))\n",
    "    gmmParameters['covMatrices'][:,:,0] = np.array([[1, 0, -3], [0, 1, 0], [-3, 0, 15]])\n",
    "    gmmParameters['covMatrices'][:,:,1] = np.array([[8, 0, 0], [0, .5, 0], [0, 0, .5]])\n",
    "    gmmParameters['covMatrices'][:,:,2] = np.array([[1, 0, -3], [0, 1, 0], [-3, 0, 15]])\n",
    "    x,labels = generateDataFromGMM(N,gmmParameters)\n",
    "    return x\n",
    "\n",
    "def generateDataFromGMM(N,gmmParameters):\n",
    "#    Generates N vector samples from the specified mixture of Gaussians\n",
    "#    Returns samples and their component labels\n",
    "#    Data dimensionality is determined by the size of mu/Sigma parameters\n",
    "    priors = gmmParameters['priors'] # priors should be a row vector\n",
    "    meanVectors = gmmParameters['meanVectors']\n",
    "    covMatrices = gmmParameters['covMatrices']\n",
    "    n = meanVectors.shape[0] # Data dimensionality\n",
    "    C = len(priors) # Number of components\n",
    "    x = np.zeros((n,N))\n",
    "    labels = np.zeros((1,N))\n",
    "    # Decide randomly which samples will come from each component\n",
    "    u = np.random.random((1,N))\n",
    "    thresholds = np.zeros((1,C+1))\n",
    "    thresholds[:,0:C] = np.cumsum(priors)\n",
    "    thresholds[:,C] = 1\n",
    "    for l in range(C):\n",
    "        indl = np.where(u <= float(thresholds[:,l]))\n",
    "        Nl = len(indl[1])\n",
    "        labels[indl] = (l+1)*1\n",
    "        u[indl] = 1.1\n",
    "        x[:,indl[1]] = np.transpose(np.random.multivariate_normal(meanVectors[:,l], covMatrices[:,:,l], Nl))\n",
    "        \n",
    "    return x,labels\n",
    "\n",
    "def plot3(a,b,c,mark=\"o\",col=\"b\"):\n",
    "  from matplotlib import pyplot\n",
    "  import pylab\n",
    "  from mpl_toolkits.mplot3d import Axes3D\n",
    "  pylab.ion()\n",
    "  fig = pylab.figure()\n",
    "  ax = Axes3D(fig)\n",
    "  ax.scatter(a, b, c,marker=mark,color=col)\n",
    "  ax.set_xlabel(\"x1\")\n",
    "  ax.set_ylabel(\"x2\")\n",
    "  ax.set_zlabel(\"y\")\n",
    "  ax.set_title('Training Dataset')\n",
    "\n",
    "def mse(x,y):\n",
    "  error=0\n",
    "  for i in range(x.shape[0]):\n",
    "    error=error+pow(x[i]-y[i],2)\n",
    "   \n",
    "  return (1/x.shape[0])*error\n",
    "\n",
    "Ntrain = 100\n",
    "data1 = generateData(Ntrain)\n",
    "#plot3(data1[0,:],data1[1,:],data1[2,:])#plot the training data\n",
    "xTrain= data1[0:2,:]\n",
    "yTrain = data1[2,:]\n",
    "\n",
    "Ntrain = 1000\n",
    "data2 = generateData(Ntrain)\n",
    "#plot3(data2[0,:],data2[1,:],data2[2,:])#plot the validation data\n",
    "xValidate = data2[0:2,:]\n",
    "yValidate = data2[2,:]\n",
    "sigm=0.1\n",
    "\n",
    "z=np.vstack((pow(xTrain,3),pow(xTrain,2),pow(xTrain,1),pow(xTrain,0)))  #design matrix\n",
    "\n",
    "gamma_list=[]\n",
    "\n",
    "for i in np.logspace(-4,4,100):\n",
    "  gamma_list.append(i)\n",
    "\n",
    "zzt=np.zeros((8,8))\n",
    "b=np.zeros(8).reshape(8,1)\n",
    "for j in range(z.shape[1]):\n",
    "  temp=z[:,j].reshape(8,1)\n",
    "  zzt=zzt+np.matmul(temp,temp.T) \n",
    "  b=b+temp*(yTrain[j])\n",
    "theta_map_list=[]\n",
    "\n",
    "theta_ml=np.matmul(np.linalg.inv(zzt+0.0001*np.random.normal(0,sigm,(8,8))),b)# I added the small error to make the matrix non-singular\n",
    "\n",
    "sigmaa=0\n",
    "for i in range(xTrain.shape[1]):\n",
    "  temp=np.matmul(theta_ml[0:2,0].reshape(1,2), pow(xTrain[:,i],3)) + np.matmul(theta_ml[2:4,0].reshape(1,2), pow(xTrain[:,i],2))+np.matmul(theta_ml[4:6,0].reshape(1,2), pow(xTrain[:,i],1))\\\n",
    "  +np.matmul(theta_ml[6:8,0].reshape(1,2), pow(xTrain[:,i],0))\n",
    "  sigmaa=sigmaa+pow((yTrain[i]-temp),2)\n",
    "\n",
    "sigmaa=(1/xTrain.shape[1])*sigmaa\n",
    "\n",
    "\n",
    "for gamma in gamma_list:\n",
    "  A=zzt+(sigmaa/pow(gamma,2))*np.eye(8)\n",
    "  theta_map=np.matmul(np.linalg.inv(A),b)\n",
    "  \n",
    "  theta_map_list.append(theta_map)\n",
    "  \n",
    "v = np.random.normal(0,np.sqrt(sigmaa),1000)\n",
    "y_validate_ml=np.matmul(theta_ml[0:2,0].reshape(1,2),pow(xValidate,3)) + np.matmul(theta_ml[2:4,0].reshape(1,2),pow(xValidate,2))\\\n",
    "  +np.matmul(theta_ml[4:6,0].reshape(1,2),pow(xValidate,1))+np.matmul(theta_ml[6:8,0].reshape(1,2),pow(xValidate,0))+v\n",
    "\n",
    "\n",
    "y_validate_map=[]\n",
    "l2_norm=[]\n",
    "l2a,l2b,l2c,l2d=[],[],[],[]\n",
    "y_train_map=[]\n",
    "\n",
    "for i in theta_map_list:\n",
    "  y_val=np.matmul(i[0:2,0].reshape(1,2),pow(xValidate,3)) + np.matmul(i[2:4,0].reshape(1,2),pow(xValidate,2))\\\n",
    "  +np.matmul(i[4:6,0].reshape(1,2),pow(xValidate,1))+np.matmul(i[6:8,0].reshape(1,2),pow(xValidate,0))\n",
    "\n",
    "  y_validate_map.append(y_val)\n",
    "  l2_norm.append(pow(np.linalg.norm(theta_ml-i),2))\n",
    "  y_train_map.append(np.matmul(i[0:2,0].reshape(1,2),pow(xTrain,3)) + np.matmul(i[2:4,0].reshape(1,2),pow(xTrain,2))\\\n",
    "  +np.matmul(i[4:6,0].reshape(1,2),pow(xTrain,1))+np.matmul(i[6:8,0].reshape(1,2),pow(xTrain,0)))\n",
    "\n",
    "\n",
    "mse_list_map=[] \n",
    "mse_list_map_train=[]\n",
    "for y in y_validate_map:\n",
    "  rr=mse(y.T,yValidate.T)\n",
    "  mse_list_map.append(rr)\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "d=[]\n",
    "for i in theta_map_list:\n",
    "  a.append(i[0:2,0])\n",
    "  b.append(i[2:4,0])\n",
    "  c.append(i[4:6,0])\n",
    "  d.append(i[6:8,0])\n",
    "\n",
    "for xx in y_train_map:\n",
    "  mse_list_map_train.append(mse(xx.T,yTrain.T))\n",
    "\n",
    "mse_list_ml=mse(yValidate.T,y_validate_ml.T)\n",
    "\n",
    "from numpy.lib.function_base import average\n",
    "print('mse for using the ml estimator:{}'.format(mse_list_ml))\n",
    "print('avergae mse for using the map estimator for different gamma values:{}'.format(average(mse_list_map)))\n",
    "print('minimum map mse:{}'.format(min(mse_list_map)))\n",
    "print(gamma_list[np.argmin(mse_list_map)])\n",
    "\n",
    "plt.plot(gamma_list,l2_norm,label='l2 norm')\n",
    "plt.plot(gamma_list,np.zeros(len(gamma_list)),'--',label='reference')\n",
    "plt.xscale(value='log')\n",
    "plt.yscale(value='log')\n",
    "plt.xlabel('gamma values')\n",
    "plt.ylabel('l2 norm between theta map and ml')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gamma_list,mse_list_map,label='mse of y_validate with MAP estimator')\n",
    "#plt.plot(gamma_list,mse_list_map_train,label='mse of y_train map with gamma values')\n",
    "plt.plot(gamma_list,[mse_list_ml for i in range(len(gamma_list))],label='mse of y_validate with ML estimator ')\n",
    "plt.plot(gamma_list[np.argmin(mse_list_map)],min(mse_list_map),'xr',label='minimum MAP estimate mse')\n",
    "plt.xscale(value='log')\n",
    "plt.yscale(value='log')\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('mean squared error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(gamma_list,a)\n",
    "plt.plot(gamma_list,b)\n",
    "plt.plot(gamma_list,c)\n",
    "plt.plot(gamma_list,d)\n",
    "plt.plot(gamma_list,np.zeros(len(gamma_list)),'--',label='reference')\n",
    "plt.xscale(value='log')\n",
    "#plt.yscale(value='log')\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel(r'MAP estimated parameters ($\\theta$)')\n",
    "plt.legend([r'${\\theta}_{00}$',r'${\\theta}_{01}$',r'${\\theta}_{10}$',r'${\\theta}_{11}$',r'${\\theta}_{20}$',r'${\\theta}_{21}$',r'${\\theta}_{30}$',r'${\\theta}_{31}$'])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
