{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from typing import Iterable\n",
    "#ERM classifier\n",
    "\n",
    "features = 4\n",
    "samples = 10000\n",
    "\n",
    "mean_0 = np.array([-1, -1, -1, -1])\n",
    "mean_1 = np.array([1, 1, 1, 1])\n",
    "\n",
    "cov_0 = np.array([[2, -0.5, 0.3, 0], [-0.5, 1, -0.5, 0], [0.3, -0.5, 1, 0], [0, 0, 0, 2]])\n",
    "cov_1 = np.array([[1, 0.3, -0.2, 0], [0.3, 2, 0.3, 0], [-0.2, 0.3, 1, 0], [0, 0, 0, 3]])\n",
    "\n",
    "# cov_0=np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]])# covariance matrix for Naive-Bayes classifier\n",
    "# cov_1=np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]])#covariance matrix for Naive-Bayes classifier\n",
    "\n",
    "prior = [0.7, 0.3]\n",
    "\n",
    "fpr = []  # false positive rate array\n",
    "tpr = []  # true positive rate array\n",
    "\n",
    "fpr_theory = []  # theoretical false positive rate\n",
    "tpr_theory = []  # theoretical true positive rate\n",
    "\n",
    "P_error = []\n",
    "gamma_list = []\n",
    "\n",
    "label = np.zeros((3, samples))\n",
    "label[0, :] = (np.random.uniform(0, 1, samples) >= prior[0]).astype(int)\n",
    "dataset = np.zeros((features, samples))\n",
    "for index in range(samples):\n",
    "    if label[0, index] == 0:\n",
    "        dataset[:, index] = mvn(mean=mean_0.reshape(4, ), cov=cov_0).rvs(1)\n",
    "    else:\n",
    "        dataset[:, index] = mvn(mean=mean_1.reshape(4, ), cov=cov_1).rvs(1)\n",
    "\n",
    "class0_count = float(list(label[0, :]).count(0))  # number of samples for class 0\n",
    "class1_count = float(list(label[0, :]).count(1))  # number of samples for class 1\n",
    "\n",
    "\n",
    "# Calculate the discriminant score\n",
    "logValpdf1 = np.log(mvn.pdf(dataset.T, mean=mean_1, cov=cov_1))\n",
    "logValpdf0 = np.log(mvn.pdf(dataset.T, mean=mean_0, cov=cov_0))\n",
    "discriminant_score = logValpdf1 - logValpdf0\n",
    "\n",
    "\n",
    "# Create list of threshold values\n",
    "\n",
    "tau = np.log(sorted(discriminant_score[np.array(discriminant_score[:].astype(float) >= 0)]))\n",
    "mid_tau = np.array([tau[0] - 100, (tau[0:(len(tau) - 1)] + (np.diff(tau)) / 2).tolist(), tau[len(tau) - 1] + 100])\n",
    "\n",
    "\n",
    "def flatten(lis):\n",
    "    for item in lis:\n",
    "        if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "            for x in flatten(item):\n",
    "                yield x\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "mid_tau = list(flatten(mid_tau.tolist()))\n",
    "\n",
    "for gamma in mid_tau:\n",
    "    label[1, :] = (discriminant_score >= gamma)\n",
    "    x10 = [i for i in range(label.shape[1]) if (label[1, i] == 1 and label[0, i] == 0)]\n",
    "    x11 = [i for i in range(label.shape[1]) if (label[1, i] == 1 and label[0, i] == 1)]\n",
    "    fpr.append(len(x10) / class0_count)\n",
    "    tpr.append(len(x11) / class1_count)\n",
    "    P_error.append((len(x10) / class0_count) * prior[0] + (1 - len(x11) / class1_count) * prior[1])\n",
    "\n",
    "# theoretical minimum error\n",
    "label[2, :] = (discriminant_score >= np.log(prior[1] / prior[0])).astype(int)\n",
    "x10_theory = [i for i in range(label.shape[1]) if (label[2, i] == 1 and label[0, i] == 0)]\n",
    "x11_theory = [i for i in range(label.shape[1]) if (label[2, i] == 1 and label[0, i] == 1)]\n",
    "fpr_theory.append(len(x10_theory) / class0_count)\n",
    "tpr_theory.append(len(x11_theory) / class1_count)\n",
    "min_p_error_theory = (len(x10_theory) / class0_count) * prior[0] + (1 - len(x11_theory) / class1_count) * prior[1]\n",
    "\n",
    "minimum_error = min(P_error)\n",
    "min_idx = np.argmin(P_error)\n",
    "\n",
    "\n",
    "print('Optimal threshold {}'.format(mid_tau[min_idx]))\n",
    "print('TPR at minimum probability:{}'.format(tpr[min_idx]))\n",
    "print('FPR at minimum probability:{}'.format(fpr[min_idx]))\n",
    "\n",
    "# Plot the actual data distribution\n",
    "x0 = [i for i in range(label.shape[1]) if (label[0, i] == 0)]\n",
    "x1 = [i for i in range(label.shape[1]) if (label[0, i] == 1)]\n",
    "\n",
    "plt.plot(dataset[0, x0], dataset[1, x0], '+', color='mediumvioletred')\n",
    "plt.plot(dataset[0, x1], dataset[1, x1], '.', color='c')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title(\"Actual Data distribution\")\n",
    "plt.legend(['Class 0', 'Class 1'])\n",
    "plt.show()\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, color='red')\n",
    "plt.plot(fpr[min_idx], tpr[min_idx], 'o', color='k')\n",
    "plt.plot(fpr_theory, tpr_theory, 'X')\n",
    "plt.xlabel('P_False Alarm')\n",
    "plt.ylabel('P_Correct Detection')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(['ROC', 'Experimental min Error', 'Theoretical min Error'])\n",
    "plt.show()\n",
    "\n",
    "# LDA classifier\n",
    "\n",
    "Sb = np.dot((mean_0 - mean_1), (mean_0 - mean_1).T)\n",
    "Sw = cov_0 + cov_1\n",
    "\n",
    "A = (np.linalg.inv(Sw)).dot(Sb)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "eigenvectors = eigenvectors.T\n",
    "\n",
    "w = np.array(eigenvectors[np.argmax(eigenvalues)])\n",
    "y0 = np.zeros((2, len(x0)))\n",
    "y1 = np.zeros((2, len(x1)))\n",
    "y0[0, :] = np.dot(w.T, dataset[:, x0])\n",
    "y1[0, :] = np.dot(w.T, dataset[:, x1])\n",
    "y = np.sort(np.hstack((y0[0], y1[0])))\n",
    "a = []\n",
    "\n",
    "fpr = []\n",
    "tpr = []\n",
    "Perror = []\n",
    "p_thr = []\n",
    "thery_mid_tau = []\n",
    "\n",
    "for threshold in mid_tau:\n",
    "    x00 = list((y0[0, :] >= threshold).astype(int)).count(0)\n",
    "    x01 = list((y1[0, :] >= threshold).astype(int)).count(0)\n",
    "    x10 = list((y0[0, :] >= threshold).astype(int)).count(1)\n",
    "    x11 = list((y1[0, :] >= threshold).astype(int)).count(1)\n",
    "    fpr.append(float(x10) / y0.shape[1])\n",
    "    tpr.append(float(x11) / y1.shape[1])\n",
    "    Perror.append((x10 / class0_count) * prior[0] + (1 - x11 / class1_count) * prior[1])\n",
    "\n",
    "for lists in range(len(mid_tau)):\n",
    "    thery_mid_tau.append(np.log(prior[1] / prior[0]))\n",
    "for threshold in thery_mid_tau:\n",
    "    x10_thr = list((y0[0, :] >= threshold).astype(int)).count(1)\n",
    "    x11_thr = list((y1[0, :] >= threshold).astype(int)).count(1)\n",
    "    p_thr.append((x10_thr / class0_count) * prior[0] + (1 - x11_thr / class1_count) * prior[1])\n",
    "idx=np.argmin(Perror)\n",
    "print('Minimum probability error:{}'.format(min(Perror)))\n",
    "print('TPR at min error:{}'.format(tpr[idx]))\n",
    "print('FPR at min error:{}'.format(fpr[idx]))\n",
    "\n",
    "# projected data distribution\n",
    "plt.scatter(y0[0, :], np.zeros((y0.shape[1])))\n",
    "plt.scatter(y1[0, :], np.zeros((y1.shape[1])))\n",
    "plt.legend(['Class 0', 'Class 1'])\n",
    "plt.title('fLDA projection ')\n",
    "plt.show()\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, color='red')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.plot(fpr[np.argmin(Perror)], tpr[np.argmin(Perror)], 'o', color='k')\n",
    "plt.title(\"ROC curve\")\n",
    "plt.legend(['ROC', 'Experimental min Error'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from numpy.random.mtrand import sample\n",
    "\n",
    "E = 7\n",
    "s = 0.1 * E\n",
    "A = np.random.randn(3, 3)\n",
    "a = 0.07\n",
    "\n",
    "temp = np.eye(3) + a * A\n",
    "temp2 = np.matmul(temp, temp)\n",
    "C1 = pow(s, 2) * temp2\n",
    "C2 = pow(s, 2) * np.eye(3)\n",
    "C3 = pow(s, 2) * np.eye(3)  # mixture for 3\n",
    "C4 = pow(s, 2) * np.eye(3)  # mixture for 3\n",
    "\n",
    "vertices = [(-1.0, -1.0, -1.0),\n",
    "            (-1.0, 1.0, -1.0),\n",
    "            (1.0, 1.0, -1.0),\n",
    "            (1.0, -1.0, -1.0),\n",
    "            (-1.0, -1.0, 1.0),\n",
    "            (-1.0, 1.0, 1.0),\n",
    "            (1.0, 1.0, 1.0),\n",
    "            (1.0, -1.0, 1.0)]\n",
    "\n",
    "mean_1 = np.array([vertices[0]])\n",
    "mean_2 = np.array([vertices[1]])\n",
    "mean_3 = np.array([vertices[2]])  # mixture for third label\n",
    "mean_4 = np.array([vertices[3]])  # mixture for third label\n",
    "\n",
    "prior = [0.3, 0.3, 0.4]\n",
    "\n",
    "features = 3\n",
    "samples = 10000\n",
    "\n",
    "label = np.zeros((3, samples))\n",
    "for i in range(samples):\n",
    "    p = np.random.uniform(0, 1)\n",
    "    if p >= 0.6:\n",
    "        label[0, i] = 3\n",
    "    else:  # sample from label 1\n",
    "        w = np.random.uniform(0, 1)\n",
    "        if w >= 0.5:\n",
    "            label[0, i] = 2\n",
    "        else:\n",
    "            label[0, i] = 1\n",
    "\n",
    "dataset = np.zeros((features, samples))\n",
    "for index in range(samples):\n",
    "    if label[0, index] == 1:\n",
    "        dataset[:, index] = np.random.multivariate_normal(mean_1.reshape(3, ), C1, 1)\n",
    "    elif label[0, index] == 2:\n",
    "        dataset[:, index] = np.random.multivariate_normal(mean_2.reshape(3, ), C2, 1)\n",
    "    else:  # mixture sampling\n",
    "        idd = np.random.uniform(0, 1)\n",
    "        if idd >= 0.5:\n",
    "            dataset[:, index] = np.random.multivariate_normal(mean_3.reshape(3, ), C3, 1)\n",
    "        else:\n",
    "            dataset[:, index] = np.random.multivariate_normal(mean_4.reshape(3, ), C4, 1)\n",
    "\n",
    "x1 = [i for i in range(label.shape[1]) if (label[0, i] == 1)]\n",
    "x2 = [i for i in range(label.shape[1]) if (label[0, i] == 2)]\n",
    "x3 = [i for i in range(label.shape[1]) if (label[0, i] == 3)]\n",
    "\n",
    "# Bayes Classifier\n",
    "\n",
    "class_1_data = dataset[:, x1]\n",
    "class_2_data = dataset[:, x2]\n",
    "class_3_data = dataset[:, x3]\n",
    "mean_list = [mean_1, mean_2, mean_3]\n",
    "cov_list = [C1, C2, C3]\n",
    "data_all = [class_1_data, class_2_data, class_3_data]\n",
    "\n",
    "lambda_matrix = [[0, 1, 1], [1, 0, 1], [1, 1, 0]]\n",
    "#lambda_matrix=[[0,1,10],[1,0,10],[1,1,0]] # for part B\n",
    "#lambda_matrix=[[0,1,100],[1,0,100],[1,1,0]]# for part B\n",
    "\n",
    "\n",
    "def risk(i, x, lambda_matrix):\n",
    "    tot_risk = 0\n",
    "    for j in range(3):\n",
    "        pp = np.random.uniform(0, 1)\n",
    "        if p >= 0.5:\n",
    "            mean_mixture = mean_3\n",
    "            cov_mixture = C3\n",
    "        else:\n",
    "            mean_mixture = mean_4\n",
    "            cov_mixture = C4\n",
    "        mean_list = [mean_1, mean_2, mean_mixture]\n",
    "        cov_list = [C1, C2, cov_mixture]\n",
    "        tot_risk = tot_risk + lambda_matrix[i][j] * prior[j] * mvn.pdf(x, mean_list[j][0], cov_list[j])\n",
    "    return tot_risk\n",
    "\n",
    "\n",
    "def MAP(true_clas, lambda_matrix):\n",
    "    predicted_correct = []\n",
    "    predicted_incorrect = []\n",
    "    confusion_matrix = np.zeros((3, 3))  # assuming the rows are actual and the columns as predicted\n",
    "    conf_list = [[[] for _ in range(3)] for _ in range(3)]\n",
    "    for i in (data_all[true_clas].T):\n",
    "        choice = np.argmin([risk(0, i, lambda_matrix), risk(1, i, lambda_matrix), risk(2, i, lambda_matrix)])\n",
    "        if choice == true_clas:\n",
    "            predicted_correct.append(i)\n",
    "        else:\n",
    "            predicted_incorrect.append(i)\n",
    "        conf_list[choice][true_clas].append(i)\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            confusion_matrix[i][j] = len(conf_list[i][j])\n",
    "    return predicted_correct, predicted_incorrect, confusion_matrix\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7.5))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "# True data distribution\n",
    "ax.scatter3D(dataset[0, x1], dataset[1, x1], dataset[2, x1], marker='p', label='class 1')\n",
    "ax.scatter3D(dataset[0, x2], dataset[1, x2], dataset[2, x2], marker='x', label='class 2')\n",
    "ax.scatter3D(dataset[0, x3], dataset[1, x3], dataset[2, x3], marker='d', label='class 3')\n",
    "plt.title(\"True Class Distributions\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "confusion_mat = np.zeros((3, 3))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7.5))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "for i in range(3):\n",
    "    predicted_correct, predicted_incorrect, confusion_matrix = MAP(i, lambda_matrix)\n",
    "    confusion_mat[:, i] = confusion_matrix[:, i]\n",
    "\n",
    "    correct_array = np.array(predicted_correct)\n",
    "    incorrect_array = np.array(predicted_incorrect)\n",
    "    labels = [['correct class 1', 'incorrect class 1'], ['correct class 2', 'incorrect class 2'],\n",
    "              ['correct class 3', 'incorrect class 3']]\n",
    "    markers = ['s', '^', 'o']\n",
    "    colors = ['g', 'b', 'o']\n",
    "    ax.scatter3D(correct_array[:, 0], correct_array[:, 1], correct_array[:, 2], c='g', marker=markers[i],\n",
    "                 label=labels[i][0])\n",
    "    ax.scatter3D(incorrect_array[:, 0], incorrect_array[:, 1], incorrect_array[:, 2], c='r', marker=markers[i],\n",
    "                 label=labels[i][1])\n",
    "\n",
    "plt.title(\" Predicted Class Distributions\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('confusion matrix:{}'.format(confusion_mat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wine Dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "wine_white = pd.read_csv('white.csv', sep=';')\n",
    "samples = wine_white.shape[0]\n",
    "\n",
    "dataset = []\n",
    "for i in range(11):\n",
    "    temp = wine_white.loc[wine_white['quality'] == i].to_numpy()\n",
    "    dataset.append(np.delete(temp, -1, axis=1))\n",
    "muVector = []\n",
    "sigmaVector = []\n",
    "lamdba_const = 0.1\n",
    "for j in range(11):\n",
    "    muVector.append(np.mean(dataset[j], axis=0))\n",
    "    sigmaVector.append(np.cov(dataset[j], rowvar=False) + lamdba_const * np.eye(11))\n",
    "for i in range(11):\n",
    "    if i == 0 or i == 1 or i == 2 or i == 10:\n",
    "        muVector[i] = np.zeros(11)\n",
    "        sigmaVector[i] = np.eye(11)\n",
    "\n",
    "# (prior for a given class) = (number of samples in the class) / (total number of samples)).\n",
    "\n",
    "priors = []\n",
    "for i in dataset:\n",
    "    temp = (i.shape[0]) / samples\n",
    "    priors.append(temp)\n",
    "\n",
    "lambda_matrix = (np.full((11, 11), 1))  # check the actual label\n",
    "np.fill_diagonal(lambda_matrix, [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "\n",
    "def risk(i, x, lambda_matrix):\n",
    "    tot = 0\n",
    "    for j in range(11):\n",
    "        tot = tot + lambda_matrix[i][j] * priors[j] * mvn.pdf(x, muVector[j], sigmaVector[j])\n",
    "    return tot\n",
    "\n",
    "\n",
    "def MAP(true_label, lambda_matrix):\n",
    "    predicted_correct = []\n",
    "    predicted_incorrect = []\n",
    "    confusion_matrix = np.zeros((11, 11))  # assuming the cols are actual and the rows as predicted\n",
    "    conf_list = [[[] for _ in range(11)] for _ in range(11)]\n",
    "    for i in dataset[true_label]:\n",
    "        choice = np.argmin([risk(k, i, lambda_matrix) for k in range(11)])\n",
    "        if choice == true_label:\n",
    "            predicted_correct.append(i)\n",
    "        else:\n",
    "            predicted_incorrect.append(i)\n",
    "\n",
    "        conf_list[choice][true_label].append(i)\n",
    "\n",
    "    for i in range(11):\n",
    "        for j in range(11):\n",
    "            confusion_matrix[i][j] = len(conf_list[i][j])\n",
    "    return predicted_correct, predicted_incorrect, confusion_matrix\n",
    "\n",
    "\n",
    "def PCA(X, n):\n",
    "    mean_x = X - np.mean(X, axis=0)\n",
    "\n",
    "    cov_mat = np.cov(mean_x, rowvar=False)\n",
    "\n",
    "    eigen_values, eigen_vectors = np.linalg.eigh(cov_mat)\n",
    "\n",
    "    sorted_index = np.argsort(eigen_values)[::-1]\n",
    "    sorted_eigenvalue = eigen_values[sorted_index]\n",
    "    sorted_eigenvectors = eigen_vectors[:, sorted_index]\n",
    "\n",
    "    eigenvector_subset = sorted_eigenvectors[:, 0:n]\n",
    "\n",
    "    x_pca = np.dot(eigenvector_subset.transpose(), mean_x.transpose()).transpose()\n",
    "\n",
    "    return x_pca\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = plt.axes()  # projection =\"2d\"\n",
    "\n",
    "scale = StandardScaler()\n",
    "\n",
    "confusion_mat = np.zeros((11, 11))\n",
    "\n",
    "for i in range(11):\n",
    "\n",
    "    predicted_correct, predicted_incorrect, confusion_matrix = MAP(i, lambda_matrix)  # calling the function\n",
    "    confusion_mat[:, i] = confusion_matrix[:, i]  # cols wise actual values\n",
    "    # scale the data before pca\n",
    "\n",
    "    correct_stacked = np.zeros((0, 11))\n",
    "    for j in predicted_correct:\n",
    "        correct_stacked = np.vstack((correct_stacked, j))\n",
    "    incorrect_stacked = np.zeros((0, 11))\n",
    "    for k in predicted_incorrect:\n",
    "        incorrect_stacked = np.vstack((incorrect_stacked, k))\n",
    "\n",
    "    if correct_stacked.size == 0 or incorrect_stacked.size == 0:\n",
    "        continue\n",
    "\n",
    "    correct = scale.fit_transform(correct_stacked)\n",
    "    incorrect = scale.fit_transform(incorrect_stacked)\n",
    "    # apply PCA\n",
    "    correct_reduced = PCA(correct, 2)\n",
    "    incorrect_reduced = PCA(incorrect, 2)\n",
    "\n",
    "    label = 'correct class {}'.format(i)\n",
    "    ax.scatter(correct_reduced[:, 0], correct_reduced[:, 1], c='g')\n",
    "    ax.scatter(incorrect_reduced[:, 0], incorrect_reduced[:, 1], c='r')\n",
    "\n",
    "plt.title(\" Predicted Class Distributions\")\n",
    "plt.legend(['correct labels', 'incorrect labels'])\n",
    "plt.show()\n",
    "\n",
    "# True class distribution in 2D\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = plt.axes()\n",
    "for i in dataset:\n",
    "    if i.size == 0:\n",
    "        continue\n",
    "    dt = PCA(i, 2)\n",
    "    ax.scatter(dt[:, 0], dt[:, 1], label='Class {}'.format(dataset.index(i)))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(confusion_mat)\n",
    "\n",
    "#HAR Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "train_data = (np.genfromtxt('X_train.txt', delimiter=''))\n",
    "train_label = (np.genfromtxt('y_train.txt', delimiter=''))\n",
    "\n",
    "test_data=(np.genfromtxt('X_test.txt', delimiter=''))\n",
    "test_label = (np.genfromtxt('y_test.txt', delimiter=''))\n",
    "\n",
    "\n",
    "nn = train_label.tolist()\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df['Labels'] = nn\n",
    "\n",
    "mm = test_label.tolist()\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df['Labels'] = mm\n",
    "\n",
    "\n",
    "dataset = []\n",
    "for i in range(1, 7, 1):\n",
    "    temp = train_df.loc[train_df['Labels'] == i].to_numpy()\n",
    "    dataset.append(np.delete(temp, -1, axis=1))\n",
    "samples = train_df.shape[0]\n",
    "dataset2=[]\n",
    "for i in range(1, 7, 1):\n",
    "    temp = test_df.loc[test_df['Labels'] == i].to_numpy()\n",
    "    dataset2.append(np.delete(temp, -1, axis=1))\n",
    "\n",
    "muVector = []\n",
    "sigmaVector = []\n",
    "lambda_const = 0.01\n",
    "for j in range(6):\n",
    "    muVector.append(np.mean(dataset[j], axis=0))\n",
    "    sigmaVector.append(np.cov(dataset[j], rowvar=False) + lambda_const * np.eye(561))\n",
    "\n",
    "priors = []\n",
    "for i in dataset:\n",
    "    temp = (i.shape[0]) / samples\n",
    "    priors.append(temp)\n",
    "\n",
    "lambda_matrix = (np.full((6, 6), 1))\n",
    "np.fill_diagonal(lambda_matrix, [0, 0, 0, 0, 0, 0])\n",
    "\n",
    "\n",
    "\n",
    "def risk(ii, x, cost_matrix):\n",
    "    tot_risk = 0\n",
    "    for j in range(6):\n",
    "        tot_risk = tot_risk + cost_matrix[ii][j] * priors[j] * mvn.pdf(x, muVector[j], sigmaVector[j])\n",
    "    return tot_risk\n",
    "\n",
    "\n",
    "def MAP(true_label, cost_matrix):\n",
    "    predicted_correct = []\n",
    "    predicted_incorrect = []\n",
    "    confusion_matrix = np.zeros((6, 6))  # assuming the rows are actual and the columns as predicted\n",
    "    conf_list = [[[] for _ in range(6)] for _ in range(6)]\n",
    "\n",
    "    for data in dataset2[true_label]:  # data is the row vector\n",
    "\n",
    "        predicted_class = np.argmin([risk(k, data, cost_matrix) for k in range(6)])\n",
    "\n",
    "        if predicted_class == true_label:\n",
    "            predicted_correct.append(data)\n",
    "        else:\n",
    "            predicted_incorrect.append(data)\n",
    "\n",
    "        conf_list[predicted_class][true_label].append(data)\n",
    "\n",
    "    for rows in range(6):\n",
    "        for cols in range(6):\n",
    "            confusion_matrix[rows][cols] = len(conf_list[rows][cols])\n",
    "    return predicted_correct, predicted_incorrect, confusion_matrix\n",
    "\n",
    "\n",
    "confusion_mat = np.zeros((6, 6))\n",
    "corr=[]\n",
    "incorr=[]\n",
    "for i in range(6):\n",
    "    predicted_correct, predicted_incorrect, confusion_matrix = MAP(i, lambda_matrix)  # calling the function\n",
    "    confusion_mat[:,i] = confusion_matrix[:,i]  # col wise actual values\n",
    "    corr.append(predicted_correct)\n",
    "    incorr.append(predicted_incorrect)\n",
    "\n",
    "print('confusion matrx:{}'.format(confusion_mat),'correct matrx:{}'.format(corr),'incorrect matrx:{}'.format(incorr))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
