{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Question 1\n",
    "import numpy as np\n",
    "from scipy . stats import multivariate_normal as mvn\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def data(n):\n",
    "  label = np.zeros((1,n))\n",
    "  for i in range(n):\n",
    "    label[0,i]= np.random.choice(np.arange(4),p=prior)\n",
    "    \n",
    "  x = np.zeros ((features , n))\n",
    "\n",
    "  for index in range (n) :\n",
    "    if label [0,index] == 0:\n",
    "      x [:,index] = mvn(m0.reshape(3,),C0).rvs(1)\n",
    "    elif label[0,index]==1:\n",
    "      x [:,index] = mvn(m1.reshape(3,),C1).rvs(1) \n",
    "    elif label[0,index]==2:\n",
    "      x [:,index] = mvn(m2.reshape(3,),C2).rvs(1) \n",
    "    else:\n",
    "      x [:,index] = mvn(m3.reshape(3,),C3).rvs(1) \n",
    "\n",
    "  return x,label # return Data_concat\n",
    "  \n",
    "# theoretical optimal classifier\n",
    "def theoretical_classifier(sample_type):\n",
    "  data_opt,label_opt=data(sample_type) # sample_type=samples[6] for the test dataset\n",
    "  lik=np.zeros((num_class,data_opt.shape[1]))\n",
    "  tot_lik=np.zeros((num_class,data_opt.shape[1]))\n",
    "  loss_mat=np.array([[0,1,1,1],[1,0,1,1],[1,1,0,1],[1,1,1,0]])\n",
    "  for label in range(num_class):\n",
    "    samp=mvn.pdf(data_opt.T,mean=mu_vector[label],cov=sigma_vector[label])\n",
    "    lik[label]=samp\n",
    "  prod=np.matmul(prior.reshape(1,num_class),lik)\n",
    "\n",
    "  for label in range(num_class):\n",
    "    tot_lik[label]=prod\n",
    "\n",
    "  temp = prior.reshape(-1, 1)*lik/tot_lik \n",
    "  risk_mat = np.matmul(loss_mat, temp)\n",
    "\n",
    "  label_sel=np.argmin(risk_mat,axis=0)\n",
    "\n",
    "  correct_samp=[]\n",
    "  incorrect_samp=[]\n",
    "\n",
    "  for label in range(num_class):\n",
    "    lbl=(label_opt==label)\n",
    "    corr_lbl=((lbl)*(label_sel==lbl)).astype('int')\n",
    "    incorr_lbl=((lbl)*(label_sel!=lbl)).astype('int')\n",
    "    correct_=np.where(corr_lbl==1)[0]\n",
    "    incorrect_=np.where(incorr_lbl==1)[0]\n",
    "    \n",
    "    correct_samp.append(correct_)\n",
    "    incorrect_samp.append(incorrect_)\n",
    "\n",
    "  p_err=1.0*np.sum((label_sel!=label_opt).astype('int'))/data_opt.shape[1]\n",
    "  print(f'Theoretical Minimum Probability of Error:{p_err.round(4)}')\n",
    "  \n",
    "def split_data(data, folds):\n",
    "  data_split = []\n",
    "  random.seed(1)\n",
    "  data_temp = list(data)\n",
    "  fold_size = int(len(data) / folds)\n",
    "  for i in range(folds):\n",
    "    fold = list()\n",
    "    while len(fold) < fold_size:\n",
    "      rand_index = random.randrange(len(data_temp))\n",
    "      fold.append(data_temp.pop(rand_index))\n",
    "    data_split.append(fold)\n",
    "  return data_split\n",
    "\n",
    "def test_train_split(folds,i):\n",
    "  test=np.array(folds.pop(i)) \n",
    "  train=np.array(folds)\n",
    "  xtest=test[:,0:3]\n",
    "  ytest=test[:,3]\n",
    "  xtrain=train[:,:,0:3].reshape(-1,3)\n",
    "  ytrain=train[:,:,3].reshape(-1)\n",
    "\n",
    "  return xtrain,ytrain,xtest,ytest\n",
    "\n",
    "def perceptrons_sel(sample_type):\n",
    "\n",
    "  error_mat=np.zeros((len(percept_list),len(activations)))\n",
    "  accuracy_mat=np.zeros((len(percept_list),len(activations)))\n",
    "\n",
    "  weights_perceptron_layer1=[]\n",
    "  weights_perceptron_layer2=[]\n",
    "  data_train,label_train=data(sample_type)\n",
    "  data_concat=np.hstack((data_train.T,label_train.T))\n",
    "\n",
    "  for activ in range(len(activations)):\n",
    "\n",
    "    activ_weight1=[]\n",
    "    activ_weight2=[]\n",
    "\n",
    "    for idx,percpt in enumerate(percept_list):\n",
    "      err_fold=[]\n",
    "      acc_fold=[]\n",
    "      \n",
    "      for i in range(num_folds):\n",
    "        X_train,Y_train,X_test,Y_test=test_train_split(split_data(data_concat,num_folds),i)\n",
    "        input_shape = (features,)    \n",
    "        Y_train_encod=to_categorical(Y_train,num_class)  \n",
    "        model = tf.keras.models.Sequential()\n",
    "\n",
    "        model.add(Dense(percpt,kernel_initializer='random_uniform', input_shape=input_shape, activation=activations[activ]))\n",
    "        model.add(Dense(num_class, kernel_initializer='random_uniform', activation='softmax'))\n",
    "\n",
    "        model.compile( optimizer='Adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(X_train,Y_train_encod, epochs=10, batch_size=10, verbose=0) \n",
    "        temp_y=model.predict(X_test) \n",
    "        y_pred=np.argmax(temp_y, axis=1)\n",
    "        tmt=to_categorical(Y_test,num_class)\n",
    "        y_test=np.argmax(tmt, axis=1)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred,labels=[0.0,1.0,2.0,3.0])\n",
    "        temp=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3])/cm.sum()\n",
    "        err_fold.append(1-temp)\n",
    "        acc_fold.append(temp)\n",
    "\n",
    "      error_mat[idx][activ]=(np.mean(err_fold)) # choose the perceptron based on the min of error\n",
    "      accuracy_mat[idx][activ]=(np.mean(acc_fold))\n",
    "\n",
    "      activ_weight1.append((model.layers[0].get_weights())[0])\n",
    "      activ_weight2.append((model.layers[1].get_weights())[0])\n",
    "    weights_perceptron_layer1.append(activ_weight1)\n",
    "    weights_perceptron_layer2.append(activ_weight2)\n",
    "\n",
    "  # test the new data \n",
    "   # find the perceptron with minimum error or loss\n",
    "  opt_percerptron=np.where(error_mat==error_mat.flat[np.argmin(error_mat)])\n",
    "  optimal_perceptron=percept_list[int(opt_percerptron[0])]\n",
    "  optimal_activation=activations[int(opt_percerptron[1])]\n",
    "\n",
    "  print(f'activation:{optimal_activation}')\n",
    "  print(f'optimal number of neurons:{optimal_perceptron}')\n",
    "\n",
    "  init1=  tf.constant_initializer(weights_perceptron_layer1[int(opt_percerptron[1])][int(opt_percerptron[0])])\n",
    "  init2=  tf.constant_initializer(weights_perceptron_layer2[int(opt_percerptron[1])][int(opt_percerptron[0])])\n",
    "\n",
    "\n",
    "  data_test,label_test=data(samples[6]) # test dataset\n",
    "  X_train_=data_train.T\n",
    "  X_test_=data_test.T\n",
    "\n",
    "  Y_train_ = to_categorical(label_train.T,num_class)\n",
    "  Y_test_ = to_categorical(label_test.T, num_class)\n",
    "\n",
    "  X_train_ = X_train_.reshape(X_train_.shape[0], features)\n",
    "  X_test_ = X_test_.reshape(X_test_.shape[0], features)\n",
    "\n",
    "  input_shape = (features,)\n",
    "  print(f'Feature shape: {input_shape}')\n",
    "\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(Dense(optimal_perceptron,kernel_initializer=init1, input_shape=input_shape, activation=optimal_activation))\n",
    "  model.add(Dense(num_class, kernel_initializer=init2,activation='softmax'))\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  model.fit(X_train_, Y_train_, epochs=10, batch_size=32, verbose=1) \n",
    "\n",
    " \n",
    "  test_results = model.evaluate(X_test_, Y_test_, verbose=1)\n",
    "  print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}')\n",
    "\n",
    "  y_pred_=model.predict(X_test_) \n",
    "  y_pred_=np.argmax(y_pred_, axis=1)\n",
    "  y_test_=np.argmax(Y_test_, axis=1)\n",
    "  cm = confusion_matrix(y_test_, y_pred_)\n",
    "  normalized_cm=cm / cm.astype(float).sum(axis=1)\n",
    "  error = (1- (cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3])/cm.sum())\n",
    "\n",
    "  print(f'minimum probability of error:{error}')\n",
    "  print(f'confusion matrix:{cm}')\n",
    "  print(f'Normalized confusion matrix:{normalized_cm}')\n",
    "  \n",
    "  return data_train,label_train,error_mat\n",
    "\n",
    "def plot_results(sample_type):\n",
    "  dataset_train,labels_training,error_matt=perceptrons_sel(sample_type)\n",
    "  #plot actual data distribution\n",
    "  actual_data=np.array(dataset_train.T)\n",
    "  actual_label=np.array(labels_training.T)\n",
    "\n",
    "  x0=[actual_data[i] for i in range(sample_type) if actual_label[i]==0]\n",
    "  x1=[actual_data[i] for i in range(sample_type) if actual_label[i]==1]\n",
    "  x2=[actual_data[i] for i in range(sample_type) if actual_label[i]==2]\n",
    "  x3=[actual_data[i] for i in range(sample_type) if actual_label[i]==3]\n",
    "\n",
    "  fig = plt.figure(figsize = (10, 7))\n",
    "  ax = plt.axes(projection =\"3d\")\n",
    "\n",
    "  ax.scatter3D((np.array(x0))[:,0],(np.array(x0))[:,1],(np.array(x0))[:,2])\n",
    "  ax.scatter3D((np.array(x1))[:,0],(np.array(x1))[:,1],(np.array(x1))[:,2])\n",
    "  ax.scatter3D((np.array(x2))[:,0],(np.array(x2))[:,1],(np.array(x2))[:,2])\n",
    "  ax.scatter3D((np.array(x3))[:,0],(np.array(x3))[:,1],(np.array(x3))[:,2])\n",
    "  plt.legend(['class0','class 1','class 2','class 3'])\n",
    "  plt.title('Actual Data Distribution')\n",
    "  plt.show\n",
    "\n",
    "  #plot min error vs number of perceptrons\n",
    "\n",
    "  tt=np.array(percept_list)\n",
    "  bar1=error_matt[:,0]\n",
    "  bar2=error_matt[:,1]\n",
    "  bar3=error_matt[:,2]\n",
    "\n",
    "  fig = plt.figure(figsize = (10, 7))\n",
    "  ax2 = plt.axes()\n",
    "  ax2.scatter(tt,bar1)\n",
    "  ax2.plot(tt,bar1)\n",
    "\n",
    "  ax2.scatter(tt,bar2)\n",
    "  ax2.plot(tt,bar2) \n",
    "\n",
    "  ax2.scatter(tt,bar3)\n",
    "  ax2.plot(tt,bar3)\n",
    "\n",
    "  plt.legend(activations)\n",
    "  plt.xlabel('Number of perceptrons') \n",
    "  plt.ylabel('Average minimum Probability of Error') \n",
    "  plt.title('Minimum Error for different number of neurons')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "  plot_results(samples[0])\n",
    "  theoretical_classifier(samples[6])\n",
    "if __name__ == \"__main__\":\n",
    "  prior=np.array([0.25,0.25,0.25,0.25])\n",
    "  features=3\n",
    "  num_class=4\n",
    "  samples = [100,200,500,1000,2000,5000,100000]\n",
    "  m0 = np.array([20, 0, 0])\n",
    "  m1 = np.array([0, 5, 10])\n",
    "  m2 = np.array([5, 0, 15])\n",
    "  m3 = np.array([20, 15,5])\n",
    "\n",
    "  mu_vector = [m0, m1, m2, m3]\n",
    "\n",
    "  C0 = np.array([[10, 0, 10], [5, 40, 0], [5, 0, 20]])\n",
    "  C1 = np.array([[40, 0, 0], [0, 25, 0], [0, 0, 10]])\n",
    "  C2 = np.array([[10, 0, 0], [0, 20, 0], [0, 0, 30]])\n",
    "  C3 = np.array([[15, 0, 0], [0, 20, 10], [0, 0, 25]])\n",
    "  sigma_vector = [C0, C1, C2, C3]\n",
    "\n",
    "  activations=['relu','elu','sigmoid']\n",
    "  num_folds=10\n",
    "  percept_list=np.logspace(1,3,num = 20,endpoint = True,base = 5,dtype = int)\n",
    "  percept_list=percept_list.tolist()\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Question 2\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy . stats import multivariate_normal as mvn\n",
    "\n",
    "\n",
    "def data(n):\n",
    "  label = np.zeros((1,n))\n",
    "\n",
    "  for i in range(n):\n",
    "    label[0,i]=np.random.choice(np.arange(4),p=prior)\n",
    "\n",
    "  x = np.zeros ((features , n))\n",
    "  for i in range (n) :\n",
    "    if label[0,i]==0:\n",
    "      x [:,i] = mvn(m0.reshape(2,),C0).rvs(1)\n",
    "    elif label[0,i]==1:\n",
    "      x [:,i] = mvn(m1.reshape(2,),C1).rvs(1)\n",
    "    elif label[0,i]==2:\n",
    "      x [:,i] = mvn(m2.reshape(2,),C2).rvs(1)\n",
    "    else:\n",
    "      x [:,i] = mvn(m3.reshape(2,),C3).rvs(1) \n",
    " \n",
    "  return x,label \n",
    "\n",
    "def model_order(sample_type):# model order selection\n",
    "  pos_log=[]\n",
    "  rates=[]\n",
    "  for k in range(num_expt):\n",
    "    Data_train,label=data(sample_type)\n",
    "    gmm_avg=np.zeros((len(gmm_list)))\n",
    "    gmm_var=np.zeros((len(gmm_list)))\n",
    "    temp=[]\n",
    "    for idx,i in enumerate(gmm_list):\n",
    "      gmm=GaussianMixture(i,covariance_type='full',random_state=None)\n",
    "      scor=cross_val_score(gmm,Data_train.T,cv=num_fold) \n",
    "      avg_scor=np.mean(scor)\n",
    "      var_scor=np.std(scor)\n",
    "\n",
    "      gmm_avg[idx]=avg_scor\n",
    "      gmm_var[idx]=var_scor\n",
    "      \n",
    "      gmm.fit(Data_train.T)\n",
    "      temp.append(avg_scor)\n",
    "      \n",
    "    model_sel=np.argmax(gmm_avg) + 1\n",
    "    model_sel_list[k]=int(model_sel)\n",
    "    pos_log.append(temp)\n",
    "\n",
    "# calculating the rates\n",
    "  for i in range(len(gmm_list)):\n",
    "    temp_list=(list(model_sel_list).count(i+1))/num_expt\n",
    "    rates.append(round(temp_list,2)) \n",
    "\n",
    "  print(f'rates_{sample_type}:{rates}' )\n",
    "  return model_sel_list,pos_log\n",
    "\n",
    "def plot_results(sample_type):\n",
    "\n",
    "  #plot the actual data distribution\n",
    "  dataset_train,labels_training=data(sample_type) \n",
    "  actual_data=np.array(dataset_train.T)\n",
    "  actual_label=np.array(labels_training.T)\n",
    "\n",
    "  x0=[actual_data[i] for i in range(sample_type) if actual_label[i]==0]\n",
    "  x1=[actual_data[i] for i in range(sample_type) if actual_label[i]==1]\n",
    "  x2=[actual_data[i] for i in range(sample_type) if actual_label[i]==2]\n",
    "  x3=[actual_data[i] for i in range(sample_type) if actual_label[i]==3]\n",
    "\n",
    "  plt.scatter((np.array(x0))[:,0],(np.array(x0))[:,1])\n",
    "  plt.scatter((np.array(x1))[:,0],(np.array(x1))[:,1])\n",
    "  plt.scatter((np.array(x2))[:,0],(np.array(x2))[:,1])\n",
    "  plt.scatter((np.array(x3))[:,0],(np.array(x3))[:,1])\n",
    "  plt.legend(['class0','class 1','class 2','class 3'])\n",
    "  plt.title('Actual Data Distribution')\n",
    "  plt.show\n",
    "\n",
    "#second plot\n",
    "  selected_list,pos_list=model_order(sample_type)\n",
    "  n_bins=len(gmm_list)\n",
    "  fig,ax=plt.subplots(tight_layout=True)\n",
    "  ax.set_xlim([1,6.5])\n",
    "  ax.hist(selected_list,bins=n_bins,color='saddlebrown')\n",
    "  plt.xlabel('model order')\n",
    "  plt.ylabel('frequency of model order')\n",
    "  plt.title('Frequency of model orders selected')\n",
    "  plt.show()\n",
    "\n",
    "#third plot\n",
    "  vv=np.array(pos_list) # return the pos_log\n",
    "  loss_=[np.mean(vv[:,i]) for i in range(vv.shape[1]) ]\n",
    "  plt.plot(np.arange(1,len(gmm_list)+1),loss_,c='b', mfc='red',marker='o')\n",
    "  plt.xlabel('model order')\n",
    "  plt.ylabel(f'log likelihood')\n",
    "  plt.title(f'log likelihood for dataset_{sample_type} under different GMM model orders',fontsize=12.)\n",
    "  plt.show()\n",
    "\n",
    "def main():\n",
    "  plot_results(samples[0])\n",
    "  plot_results(samples[1])\n",
    "  plot_results(samples[2])\n",
    "  plot_results(samples[3])\n",
    "  \n",
    "     \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    prior=np.array([0.1,0.2,0.3,0.4])\n",
    "    features=2\n",
    "    num_class=4\n",
    "    samples = [10,100,1000,10000]\n",
    "    m0 = np.array([0,50])\n",
    "    m1 = np.array([30,50])\n",
    "    m2 = np.array([0,0])\n",
    "    m3 = np.array([30, 0])\n",
    "\n",
    "    mu_vector = [m0, m1, m2, m3]\n",
    "\n",
    "    C0 = np.array([[10, 0], [10, 20]])\n",
    "    C1 = np.array([[20, 30], [0, 10]])\n",
    "    C2 = np.array([[20, 30], [0, 10]])\n",
    "    C3 = np.array([[10, 20], [0, 40]]) \n",
    "    \n",
    "    sigma_vector = [C0, C1, C2, C3]\n",
    "\n",
    "    gmm_list=[1,2,3,4,5,6]\n",
    "    num_expt=30\n",
    "    num_fold=10\n",
    "    model_sel_list=np.zeros((num_expt))\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
